{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_colorization_training_code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiKZTyGixAHW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO-gZ7ILxA7c"
      },
      "source": [
        "Import the libraries :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weP0Oc_ExG2i"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.color import rgb2lab, lab2rgb, gray2rgb\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKJEAoExVBe"
      },
      "source": [
        "Import  pretrained VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKBNc6pxdFK"
      },
      "source": [
        "vgg16 = VGG16()\n",
        "vgg_up_to_19th = Sequential() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcxGPuuAxt_9"
      },
      "source": [
        "Take the encoder part (first 19 layers) of VGG16 model and add the layes into our sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq4eRvXFyNlC"
      },
      "source": [
        "for i, layer in enumerate(vgg16.layers):\n",
        "    if i<19:          \n",
        "      vgg_up_to_19th.add(layer)\n",
        "vgg_up_to_19th.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG3ru6EKyU0X"
      },
      "source": [
        "we want the layers of VGG16 with its original weights without changing them,so that we set the trainable parameter in each layer to false because we donâ€™t want to train them again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvTcjyhvyl8c"
      },
      "source": [
        "for layer in vgg_up_to_19th.layers:\n",
        "  layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq10KNcLy27M"
      },
      "source": [
        "Initialize the path of the training images folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBNA3iMzDa-"
      },
      "source": [
        "path = '/content/drive/MyDrive/project work/human dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnMt_WsxzPoK"
      },
      "source": [
        "Normalize the images - divide by 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZeKVVIPzZ_w"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1. / 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hEZ-QQ-zhkp"
      },
      "source": [
        "VGG16 is expecting an image of 3 dimension with size 224x224 as an input, \n",
        "in preprocessing we have to scale all images to 224"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqEoDuv6zyG3"
      },
      "source": [
        "train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=1500,class_mode=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuL0d-a0NWN"
      },
      "source": [
        "By iterating on each image, we convert the RGB to Lab,where L channel repressents lightness and a,b channel represent the color of the images. So,we can consider the L channel as greyscale image and a,b channel as the colorized version of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_noMQfy1Dnq"
      },
      "source": [
        "X =[]\n",
        "Y =[]\n",
        "\n",
        "for img in train[0]:\n",
        "  try:\n",
        "      lab = rgb2lab(img)\n",
        "      X.append(lab[:,:,0])\n",
        "      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128,\n",
        "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
        "  except:\n",
        "    print('error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niHrRGMu1czg"
      },
      "source": [
        "Convert X and Y into numpy arrays and change their dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqBLhFT1vn1"
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X = X.reshape(X.shape+(1,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ2qW2i14AH"
      },
      "source": [
        "The L channel of the image has only 1 dimension but vgg16 needs 3 dimensions to work .So we are converting the L channel grey image to rgb to add extra 2 layers.\n",
        "Then We are passing that image to our vgg16 model for prediction.And the prediction result will be used as a input to the decoder which is created by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq6LYLgR7k3u"
      },
      "source": [
        "vggfeatures = []\n",
        "for i, sample in enumerate(X):\n",
        "  sample = gray2rgb(sample)\n",
        "  sample = sample.reshape((1,224,224,3))\n",
        "  prediction = vgg_up_to_19th.predict(sample)\n",
        "  prediction = prediction.reshape((7,7,512))\n",
        "  vggfeatures.append(prediction)\n",
        "vggfeatures = np.array(vggfeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlGdSkBo8Os_"
      },
      "source": [
        "Adding decoding layers to our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ59FaS8WVJ"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same', input_shape=(7,7,512)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEqGnkCy8d7W"
      },
      "source": [
        "Train the model and save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-pAl7iP8rLw"
      },
      "source": [
        "tf_callbacks=tf.keras.callbacks.TensorBoard(log_dir='logs/',histogram_freq=1)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse' , metrics=['accuracy'])\n",
        "model.fit(vggfeatures, Y, verbose=1, epochs=1000, batch_size=40,callbacks=[tf_callbacks])\n",
        "model.save('human_model.model',save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}